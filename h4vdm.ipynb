{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "import torch\n",
    "import wandb as wb\n",
    "from packages.video_utils import H264Extractor, Video\n",
    "from packages.constants import GOP_SIZE, FRAME_HEIGHT, FRAME_WIDTH, DATASET_ROOT, N_GOPS_FROM_DIFFERENT_DEVICE, N_GOPS_FROM_SAME_DEVICE\n",
    "from packages.dataset import VisionGOPDataset, GopPairDataset\n",
    "from packages.common import create_custom_logger\n",
    "from packages.network import H4vdmNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_ROOT):\n",
    "    raise Exception(f'Dataset root does not exist: {DATASET_ROOT}')\n",
    "\n",
    "log = create_custom_logger('h4vdm.ipynb')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log.info(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GOP dataset\n",
    "\n",
    "Remember to delete dataset.json if you want to add new devices/videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = os.path.abspath(os.path.join(os.getcwd(), 'h264-extractor', 'bin'))\n",
    "h264_ext_bin = os.path.join(bin_path, 'h264dec_ext_info')\n",
    "h264_extractor = H264Extractor(bin_filename=h264_ext_bin, cache_dir=DATASET_ROOT)\n",
    "Video.set_h264_extractor(h264_extractor)\n",
    "\n",
    "dataset = VisionGOPDataset(\n",
    "    root_path=DATASET_ROOT,\n",
    "    devices=[],\n",
    "    media_types = ['videos'],\n",
    "    properties=['flat'],\n",
    "    extensions=['mp4'],\n",
    "    gop_size=GOP_SIZE,\n",
    "    frame_width=FRAME_WIDTH,\n",
    "    frame_height=FRAME_HEIGHT,\n",
    "    gops_per_video=4,\n",
    "    build_on_init=False,\n",
    "    force_rebuild=False,\n",
    "    download_on_init=False,\n",
    "    ignore_local_dataset=False,\n",
    "    shuffle=False)\n",
    "\n",
    "is_loaded = dataset.load()\n",
    "if not is_loaded:\n",
    "    log.info('Dataset was not loaded. Building...')\n",
    "else:\n",
    "    log.info('Dataset was loaded.')\n",
    "\n",
    "print(f'Dataset length: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "devices = list(dataset.get_devices())\n",
    "\n",
    "print(f'All devices: {devices}')\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_1_devices = devices[:len(devices)//2]\n",
    "training_set_1_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_2_devices = devices[:len(devices)//2]\n",
    "training_set_2_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_3_devices = devices[:len(devices)//2]\n",
    "training_set_3_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_4_devices = devices[:len(devices)//2]\n",
    "training_set_4_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_5_devices = devices[:len(devices)//3]\n",
    "training_set_5_devices = devices[len(devices)//3:]\n",
    "\n",
    "testing_set_6_devices = devices[len(devices)//3:2*(len(devices)//3)]\n",
    "training_set_6_devices = devices[:len(devices)//3] + devices[2*(len(devices)//3):]\n",
    "\n",
    "testing_set_7_devices = devices[2*(len(devices)//3):]\n",
    "training_set_7_devices = devices[:2*(len(devices)//3)]\n",
    "\n",
    "training_set_devices = [training_set_1_devices, training_set_2_devices, training_set_3_devices, training_set_4_devices, training_set_5_devices, training_set_6_devices, training_set_7_devices]\n",
    "testing_set_devices = [testing_set_1_devices, testing_set_2_devices, testing_set_3_devices, testing_set_4_devices, testing_set_5_devices, testing_set_6_devices, testing_set_7_devices]\n",
    "\n",
    "assert len(training_set_devices) == len(testing_set_devices)\n",
    "n_epochs = len(training_set_devices)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print(f'Training set {i+1} devices: {training_set_devices[i]}')\n",
    "    print(f'Testing set {i+1} devices: {testing_set_devices[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build all GOPs so that cache can be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for device in dataset.get_devices():\n",
    "#     for video_metadata in dataset.dataset[device]:\n",
    "#         video = dataset._get_video_from_metadata(video_metadata)\n",
    "#         gops = video.get_gops()\n",
    "\n",
    "#         Video.h264_extractor.clean_cache()\n",
    "#         video = None\n",
    "#         gops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 72\n",
    "LEARNING_RATE = 8e-6\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.97\n",
    "VALIDATION_PERCENTAGE = 12.5 # 1/8\n",
    "TEST_PERCENTAGE = 40\n",
    "WARM_UP_EPOCHS = 5\n",
    "START_LINEAR_LEARNING_RATE_COEFFICIENT = 1e-9 # basically zero\n",
    "END_LINEAR_LEARNING_RATE_COEFFICIENT = 1\n",
    "\n",
    "\n",
    "# define loss function\n",
    "compute_loss = torch.nn.BCELoss()\n",
    "# compute_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Check if the loss function is working\n",
    "print(f'Same / Same: {compute_loss(torch.tensor([1.0]), torch.tensor([1.0]))}')\n",
    "print(f'Same / Different: {compute_loss(torch.tensor([1.0]), torch.tensor([0.0]))}')\n",
    "print(f'Different / Different: {compute_loss(torch.tensor([0.0]), torch.tensor([0.0]))}')\n",
    "print(f'Different / Same: {compute_loss(torch.tensor([0.0]), torch.tensor([1.0]))}')\n",
    "\n",
    "# instantiate the model\n",
    "net = H4vdmNet()\n",
    "net = net.to(device)\n",
    "  \n",
    "# instantiate the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "linear_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, START_LINEAR_LEARNING_RATE_COEFFICIENT, END_LINEAR_LEARNING_RATE_COEFFICIENT, WARM_UP_EPOCHS)\n",
    "exponential_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, LEARNING_RATE_DECAY_FACTOR)\n",
    "\n",
    "def compute_similarity(gop1_features, gop2_features):\n",
    "    diff = torch.subtract(gop1_features, gop2_features)\n",
    "    norm = torch.norm(diff, 2)\n",
    "    tanh = torch.tanh(norm)\n",
    "    return (torch.ones(tanh.shape) - tanh)\n",
    "\n",
    "def train_one_step(model, gop1, gop2, label, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    gop1_features = model(gop1, debug=False, device=device)\n",
    "    gop2_features = model(gop2, debug=False, device=device)\n",
    "    gop1_features = gop1_features.to(device)\n",
    "    gop2_features = gop2_features.to(device)\n",
    "\n",
    "    similarity = compute_similarity(gop1_features, gop2_features).double()\n",
    "    similarity.to(device)\n",
    "    \n",
    "    label = torch.tensor(label, dtype=float, requires_grad=False, device=device)\n",
    "    label = label.double()\n",
    "    \n",
    "    loss = compute_loss(similarity, label)\n",
    "    loss = loss.to(device)\n",
    "    # print(f'Iteration {i}/{len(dataset)} | \\tLabel: {label} - Similarity: {similarity} - Loss: {loss}')\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model, dataset, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    wb.log({\"learning-rate\": scheduler.get_last_lr()[0]})\n",
    "    for i in range(len(dataset)):\n",
    "        gop1, gop2, label = dataset[i]\n",
    "        loss = train_one_step(model, gop1, gop2, label, optimizer)\n",
    "        total_loss += loss.item()\n",
    "        wb.log({\"instantaneous-loss\": loss.item()})\n",
    "        wb.log({\"total-loss\": total_loss})\n",
    "    \n",
    "    print(f'Updating learning rate from {scheduler.get_last_lr()[0]}', end='')\n",
    "    scheduler.step()\n",
    "    print(f' to {scheduler.get_last_lr()[0]}')\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "def test(dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GOPS_FROM_DIFFERENT_DEVICE = 5\n",
    "N_GOPS_FROM_SAME_DEVICE = 5\n",
    "\n",
    "wb.init(project='h4vdm', config={\"learning-rate\": LEARNING_RATE,\n",
    "                                 \"learning-rate-decay-factor\": LEARNING_RATE_DECAY_FACTOR,\n",
    "                                 \"start-linear-learning-rate-coefficient\": START_LINEAR_LEARNING_RATE_COEFFICIENT,\n",
    "                                 \"end-linear-learning-rate-coefficient\": END_LINEAR_LEARNING_RATE_COEFFICIENT,\n",
    "                                 \"n-warmup-epochs\": WARM_UP_EPOCHS,\n",
    "                                 \"n_epochs\": n_epochs,\n",
    "                                 \"n-gops-from-same-device\": N_GOPS_FROM_SAME_DEVICE,\n",
    "                                 \"n-gops-from-different-device\": N_GOPS_FROM_DIFFERENT_DEVICE,\n",
    "                                 \"validation-percentage\": VALIDATION_PERCENTAGE,\n",
    "                                 \"test-percentage\": TEST_PERCENTAGE,\n",
    "                                 \"batch-size\": BATCH_SIZE})\n",
    "\n",
    "for i in range(WARM_UP_EPOCHS):\n",
    "    print(f'Warmup epoch {i+1}/{WARM_UP_EPOCHS}')\n",
    "    print(f'Loading training set {i+1}/{n_epochs}')\n",
    "    training_set = GopPairDataset(dataset, N_GOPS_FROM_SAME_DEVICE, N_GOPS_FROM_DIFFERENT_DEVICE, consider_devices=training_set_devices[i], shuffle=True)\n",
    "    print(f'Training epoch {i+1}/{n_epochs}')\n",
    "    train_one_epoch(net, training_set, optimizer, linear_scheduler)\n",
    "    \n",
    "    \n",
    "    # print(f'Loading testing set {i+1}/{n_epochs}')\n",
    "    # testing_set = GopPairDataset(dataset, N_GOPS_FROM_SAME_DEVICE, N_GOPS_FROM_DIFFERENT_DEVICE, consider_devices=testing_set_devices[i], shuffle=True)\n",
    "    # testing_set = testing_set[:len(training_set)*(1-TEST_PERCENTAGE/100)] # reduce size by removing TEST_PERCENTAGE % of the dataset\n",
    "    # validation_set = testing_set[:floor(len(training_set)*(1-VALIDATION_PERCENTAGE/100))] # VALIDATION_PERCENTAGE % of the training set is used for validation, works because the dataset is shuffled\n",
    "    # testing_set = testing_set[floor(len(training_set)*(1-VALIDATION_PERCENTAGE/100)):]\n",
    "    # net.eval()\n",
    "    # test(testing_set)\n",
    "    print(f'Epoch {i+1}/{n_epochs} done')\n",
    "    print('')\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('models', datetime.today().strftime('%Y-%m-%d_%H:%M') + '_h4vdm.pth')\n",
    "print(f'Saving model to {filename}')\n",
    "torch.save(net, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('2024-02-22_19-38_h4vdm.pth')\n",
    "net.eval()\n",
    "\n",
    "gop1, gop2, label = pair_dataset[10]\n",
    "gop1_features = net(gop1, debug=False, device=device)\n",
    "gop2_features = net(gop2, debug=False, device=device)\n",
    "\n",
    "similarity = compute_similarity(gop1_features, gop2_features)\n",
    "\n",
    "print(f'Gop1: {gop1.video_name} Gop2: {gop2.video_name} Label: {label} Similarity: {similarity}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
