{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from random import randint\n",
    "import skimage.color\n",
    "from packages.video_utils import H264Extractor, Video, Gop\n",
    "from packages.constants import GOP_SIZE, FRAME_HEIGHT, FRAME_WIDTH, DATASET_ROOT, MACROBLOCK_SIZE\n",
    "from packages.dataset import VisionDataset, VisionGOPDataset\n",
    "from packages.common import create_custom_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_ROOT):\n",
    "    raise Exception(f'Dataset root does not exist: {DATASET_ROOT}')\n",
    "\n",
    "log = create_custom_logger('h4vdm.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to delete dataset.json if you want to add new devices/videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = os.path.abspath(os.path.join(os.getcwd(), 'h264-extractor', 'bin'))\n",
    "h264_ext_bin = os.path.join(bin_path, 'h264dec_ext_info')\n",
    "h264_extractor = H264Extractor(bin_filename=h264_ext_bin, cache_dir=DATASET_ROOT)\n",
    "Video.set_h264_extractor(h264_extractor)\n",
    "\n",
    "dataset = VisionGOPDataset(\n",
    "    root_path=DATASET_ROOT,\n",
    "    devices=[],\n",
    "    media_types = ['videos'],\n",
    "    properties=['flat'],\n",
    "    extensions=['mp4'],\n",
    "    gop_size=GOP_SIZE,\n",
    "    frame_width=FRAME_WIDTH,\n",
    "    frame_height=FRAME_HEIGHT,\n",
    "    gops_per_video=4,\n",
    "    build_on_init=False,\n",
    "    force_rebuild=False,\n",
    "    download_on_init=False,\n",
    "    ignore_local_dataset=False,\n",
    "    shuffle=False)\n",
    "\n",
    "is_loaded = dataset.load()\n",
    "if not is_loaded:\n",
    "    log.info('Dataset was not loaded. Building...')\n",
    "else:\n",
    "    log.info('Dataset was loaded.')\n",
    "\n",
    "print(f'Dataset length: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(dataset)):\n",
    "#     video = dataset[i]\n",
    "#     print(f'{i} - {video.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(dataset)):\n",
    "#     try:\n",
    "#         video = dataset[i]\n",
    "#         # print(f'Video {i} GOPs: {len(video.get_gops())}')\n",
    "#     except Exception as e:\n",
    "#         log.error(f'Error while processing video {i}: {e}')\n",
    "#         raise e\n",
    "\n",
    "\n",
    "#     gops = video.get_gops()\n",
    "\n",
    "#     for gop in gops:\n",
    "#         print(f'Number of GOPs in video: {len(gops)}')\n",
    "#         print(f'Frame 0 shape: {gop.get_intra_frame().shape}')\n",
    "#         for i, inter_frame in enumerate(gop.get_inter_frames()):\n",
    "#             print(f'Frame {i + 1} shape: {inter_frame.shape}')\n",
    "#         print(f'Frame types {gop.get_frame_types()}')\n",
    "#         print(f'Macroblock types shape: {len(gop.get_macroblock_images()[0])} x {gop.get_macroblock_images()[0].shape}')\n",
    "#         print(f'Luma QPs {len(gop.get_luma_qp_images()[0])}')\n",
    "# dataset.save('test_dataset_after.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "# gop = gops[0]\n",
    "\n",
    "# for frame_id in range(gop.get_first_frame_number(), gop.get_first_frame_number() + len(gop)):\n",
    "#     fig.add_subplot(len(gop)//2, len(gop)//2, frame_id + 1)\n",
    "#     plt.imshow(gop.get_rgb_frame(frame_id))\n",
    "#     plt.title('Frame {}'.format(frame_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build all GOPs so that cache can be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for device in dataset.get_devices():\n",
    "#     for video_metadata in dataset.dataset[device]:\n",
    "#         video = dataset._get_video_from_metadata(video_metadata)\n",
    "#         gops = video.get_gops()\n",
    "\n",
    "#         Video.h264_extractor.clean_cache()\n",
    "#         video = None\n",
    "#         gops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAN INPUT SIZE = embed_dim = 4 * 8 + 5 = 37\n",
    "num_heads = HAN_N_HEADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.network import H4vdmNet\n",
    "logger = create_custom_logger('h4vdm.ipynb')\n",
    "from math import tanh, sqrt, log\n",
    "import torch\n",
    "from random import random\n",
    "\n",
    "def compute_similarity(gop1_features, gop2_features):\n",
    "    return 1 - tanh(torch.norm(gop1_features - gop2_features, 2))\n",
    "\n",
    "compute_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "training_dataset_paths = dataset._build_gop_pair_dataset()\n",
    "\n",
    "print(f'Training dataset length: {len(training_dataset_paths)}')\n",
    "for gop1_path, gop2_path, label in training_dataset_paths:\n",
    "    gop1 = Gop.load(gop1_path, None)\n",
    "    gop2 = Gop.load(gop2_path, None)\n",
    "    logger.info(f'Gop1: {gop1.video_name} Gop2: {gop2.video_name} Label: {label}')\n",
    "\n",
    "\n",
    "# net = H4vdmNet()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "# for gop1, gop2, label in training_dataset:\n",
    "#     label = 1 if label else 0\n",
    "\n",
    "#     gop1_features = net(gop1)\n",
    "#     gop2_features = net(gop2)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     similarity = compute_similarity(gop1_features, gop2_features)\n",
    "#     similarity = torch.tensor(similarity)\n",
    "#     label = torch.tensor(label, dtype=float)\n",
    "#     loss = compute_loss(similarity, label)\n",
    "#     loss.bakcward()\n",
    "\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(f'Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "\n",
    "# # net.forward(gop, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
