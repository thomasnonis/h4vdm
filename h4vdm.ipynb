{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from random import randint\n",
    "import skimage.color\n",
    "from packages.video_utils import H264Extractor, Video, Gop\n",
    "from packages.constants import GOP_SIZE, FRAME_HEIGHT, FRAME_WIDTH, DATASET_ROOT, MACROBLOCK_SIZE\n",
    "from packages.dataset import VisionDataset, VisionGOPDataset, GopPairDataset\n",
    "from packages.common import create_custom_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_ROOT):\n",
    "    raise Exception(f'Dataset root does not exist: {DATASET_ROOT}')\n",
    "\n",
    "log = create_custom_logger('h4vdm.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to delete dataset.json if you want to add new devices/videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = os.path.abspath(os.path.join(os.getcwd(), 'h264-extractor', 'bin'))\n",
    "h264_ext_bin = os.path.join(bin_path, 'h264dec_ext_info')\n",
    "h264_extractor = H264Extractor(bin_filename=h264_ext_bin, cache_dir=DATASET_ROOT)\n",
    "Video.set_h264_extractor(h264_extractor)\n",
    "\n",
    "dataset = VisionGOPDataset(\n",
    "    root_path=DATASET_ROOT,\n",
    "    devices=[],\n",
    "    media_types = ['videos'],\n",
    "    properties=['flat'],\n",
    "    extensions=['mp4'],\n",
    "    gop_size=GOP_SIZE,\n",
    "    frame_width=FRAME_WIDTH,\n",
    "    frame_height=FRAME_HEIGHT,\n",
    "    gops_per_video=4,\n",
    "    build_on_init=False,\n",
    "    force_rebuild=False,\n",
    "    download_on_init=False,\n",
    "    ignore_local_dataset=False,\n",
    "    shuffle=False)\n",
    "\n",
    "is_loaded = dataset.load()\n",
    "if not is_loaded:\n",
    "    log.info('Dataset was not loaded. Building...')\n",
    "else:\n",
    "    log.info('Dataset was loaded.')\n",
    "\n",
    "print(f'Dataset length: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.constants import N_GOPS_FROM_DIFFERENT_DEVICE, N_GOPS_FROM_SAME_DEVICE\n",
    "\n",
    "pair_dataset = GopPairDataset(dataset, N_GOPS_FROM_SAME_DEVICE, N_GOPS_FROM_DIFFERENT_DEVICE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build all GOPs so that cache can be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for device in dataset.get_devices():\n",
    "#     for video_metadata in dataset.dataset[device]:\n",
    "#         video = dataset._get_video_from_metadata(video_metadata)\n",
    "#         gops = video.get_gops()\n",
    "\n",
    "#         Video.h264_extractor.clean_cache()\n",
    "#         video = None\n",
    "#         gops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.network import H4vdmNet\n",
    "from math import tanh, sqrt, log\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "logger = create_custom_logger('h4vdm.ipynb')\n",
    "\n",
    "def compute_similarity(gop1_features, gop2_features):\n",
    "    return 1 - tanh(torch.norm(gop1_features - gop2_features, 2))\n",
    "\n",
    "compute_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = H4vdmNet()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# training_dataloader = DataLoader(pair_dataset, batch_size=8, shuffle=True)\n",
    "# training_dataloder_iterator = iter(training_dataloader)\n",
    "\n",
    "tot = len(pair_dataset)\n",
    "print(f'Training dataset length: {tot}')\n",
    "optimizer.zero_grad()\n",
    "for i in range(0, len(pair_dataset)):\n",
    "    gop1, gop2, label = pair_dataset[i]\n",
    "    print(f'Iteration {i}/{tot} - Gop1: {gop1.video_name} Gop2: {gop2.video_name} Label: {label}')\n",
    "\n",
    "\n",
    "    # print('Gop1 forward')\n",
    "    gop1_features = net(gop1, debug=False)\n",
    "    # print('Gop2 forward')\n",
    "    gop2_features = net(gop2, debug=False)\n",
    "\n",
    "\n",
    "    similarity = compute_similarity(gop1_features, gop2_features)\n",
    "    # print(f'Similarity before casting: {similarity}')\n",
    "    similarity = torch.tensor([similarity], dtype=float, requires_grad=True)\n",
    "    # print(f'Similarity after casting: {similarity}')\n",
    "    label = torch.tensor([label], dtype=float, requires_grad=True)\n",
    "    # print(f'Label: {label}')\n",
    "    # print(f'similarity: {type(similarity)}, label: {type(label)}')\n",
    "    # print(f'similarity: {similarity.shape}, label: {label.shape}')\n",
    "    loss = compute_loss(similarity, label)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Loss: {loss.item()}')\n",
    "    i += 1\n",
    "\n",
    "print('Done')\n",
    "\n",
    "# # net.forward(gop, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
