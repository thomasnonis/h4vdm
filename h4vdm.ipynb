{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "import copy\n",
    "import torch\n",
    "import wandb as wb\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import auc, f1_score, roc_curve\n",
    "from packages.video_utils import H264Extractor, Video\n",
    "from packages.constants import GOP_SIZE, FRAME_HEIGHT, FRAME_WIDTH, DATASET_ROOT, N_GOPS_FROM_DIFFERENT_DEVICE, N_GOPS_FROM_SAME_DEVICE, SAME_DEVICE_LABEL\n",
    "from packages.dataset import VisionGOPDataset, GopPairDataset\n",
    "from packages.common import create_custom_logger\n",
    "from packages.network import H4vdmNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_ROOT):\n",
    "    raise Exception(f'Dataset root does not exist: {DATASET_ROOT}')\n",
    "\n",
    "log = create_custom_logger('h4vdm.ipynb')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log.info(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GOP dataset\n",
    "\n",
    "Remember to delete dataset.json if you want to add new devices/videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = os.path.abspath(os.path.join(os.getcwd(), 'h264-extractor', 'bin'))\n",
    "h264_ext_bin = os.path.join(bin_path, 'h264dec_ext_info')\n",
    "h264_extractor = H264Extractor(bin_filename=h264_ext_bin, cache_dir=DATASET_ROOT)\n",
    "Video.set_h264_extractor(h264_extractor)\n",
    "\n",
    "vision_gop_dataset = VisionGOPDataset(\n",
    "    root_path=DATASET_ROOT,\n",
    "    devices=[],\n",
    "    media_types = ['videos'],\n",
    "    properties=[],\n",
    "    extensions=['mp4', 'mov', '3gp'],\n",
    "    gop_size=GOP_SIZE,\n",
    "    frame_width=FRAME_WIDTH,\n",
    "    frame_height=FRAME_HEIGHT,\n",
    "    gops_per_video=4,\n",
    "    build_on_init=False,\n",
    "    force_rebuild=False,\n",
    "    download_on_init=False,\n",
    "    ignore_local_dataset=False,\n",
    "    shuffle=False)\n",
    "\n",
    "is_loaded = vision_gop_dataset.load()\n",
    "if not is_loaded:\n",
    "    log.info('Dataset was not loaded. Building...')\n",
    "else:\n",
    "    log.info('Dataset was loaded.')\n",
    "\n",
    "print(f'Dataset length: {len(vision_gop_dataset)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "devices = list(vision_gop_dataset.get_devices())\n",
    "\n",
    "print(f'{len(devices)} devices: {devices}')\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_1_devices = devices[:len(devices)//2]\n",
    "training_set_1_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_2_devices = devices[:len(devices)//2]\n",
    "training_set_2_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_3_devices = devices[:len(devices)//2]\n",
    "training_set_3_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_4_devices = devices[:len(devices)//2]\n",
    "training_set_4_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_5_devices = devices[:len(devices)//3]\n",
    "training_set_5_devices = devices[len(devices)//3:]\n",
    "\n",
    "testing_set_6_devices = devices[len(devices)//3:2*(len(devices)//3)]\n",
    "training_set_6_devices = devices[:len(devices)//3] + devices[2*(len(devices)//3):]\n",
    "\n",
    "testing_set_7_devices = devices[2*(len(devices)//3):]\n",
    "training_set_7_devices = devices[:2*(len(devices)//3)]\n",
    "\n",
    "training_set_devices = [training_set_1_devices, training_set_2_devices, training_set_3_devices, training_set_4_devices, training_set_5_devices, training_set_6_devices, training_set_7_devices]\n",
    "testing_set_devices = [testing_set_1_devices, testing_set_2_devices, testing_set_3_devices, testing_set_4_devices, testing_set_5_devices, testing_set_6_devices, testing_set_7_devices]\n",
    "\n",
    "# training_set_devices = [training_set_1_devices]\n",
    "# testing_set_devices = [testing_set_1_devices]\n",
    "\n",
    "assert len(training_set_devices) == len(testing_set_devices), 'There must be the same number of training and testing sets'\n",
    "n_datasets = len(training_set_devices)\n",
    "\n",
    "for epoch in range(n_datasets):\n",
    "    print(f'Training set {epoch+1} devices: {training_set_devices[epoch]}')\n",
    "    print(f'Testing set {epoch+1} devices: {testing_set_devices[epoch]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build all GOPs so that cache can be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for device in vision_gop_dataset.get_devices():\n",
    "#     for video_metadata in vision_gop_dataset.dataset[device]:\n",
    "#         video = vision_gop_dataset._get_video_from_metadata(video_metadata)\n",
    "#         gops = video.get_gops()\n",
    "\n",
    "#         Video.h264_extractor.clean_cache()\n",
    "#         video = None\n",
    "#         gops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 72\n",
    "LEARNING_RATE = 8e-6\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.97\n",
    "VALIDATION_PERCENTAGE = 12.5 # 1/8\n",
    "TEST_PERCENTAGE = 40\n",
    "WARM_UP_EPOCHS = 5\n",
    "START_LINEAR_LEARNING_RATE_COEFFICIENT = 1e-3 # basically zero\n",
    "END_LINEAR_LEARNING_RATE_COEFFICIENT = 1\n",
    "TARGET_FPR = 0.01\n",
    "AUC_INCREASE_THRESHOLD = 0.01\n",
    "AUC_STABLE_COUNTER_THRESHOLD = 4\n",
    "\n",
    "\n",
    "# define loss function\n",
    "compute_loss = torch.nn.BCELoss()\n",
    "# compute_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Check if the loss function is working\n",
    "print(f'Same / Same: {compute_loss(torch.tensor([1.0]), torch.tensor([1.0]))}')\n",
    "print(f'Same / Different: {compute_loss(torch.tensor([1.0]), torch.tensor([0.0]))}')\n",
    "print(f'Different / Different: {compute_loss(torch.tensor([0.0]), torch.tensor([0.0]))}')\n",
    "print(f'Different / Same: {compute_loss(torch.tensor([0.0]), torch.tensor([1.0]))}')\n",
    "\n",
    "# instantiate the model\n",
    "net = H4vdmNet()\n",
    "net = net.to(device)\n",
    "  \n",
    "# instantiate the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def compute_similarity(gop1_features, gop2_features):\n",
    "    diff = torch.subtract(gop1_features, gop2_features)\n",
    "    norm = torch.norm(diff, 2)\n",
    "    tanh = torch.tanh(norm)\n",
    "    return (torch.ones(tanh.shape) - tanh)\n",
    "\n",
    "def train_one_step(model, gop1, gop2, label):\n",
    "    gop1_features = model(gop1, debug=False, device=device)\n",
    "    gop2_features = model(gop2, debug=False, device=device)\n",
    "    gop1_features = gop1_features.to(device)\n",
    "    gop2_features = gop2_features.to(device)\n",
    "\n",
    "    similarity = compute_similarity(gop1_features, gop2_features).double()\n",
    "    similarity.to(device)\n",
    "    \n",
    "    label = torch.tensor(label, dtype=float, requires_grad=False, device=device)\n",
    "    label = label.double()\n",
    "    \n",
    "    loss = compute_loss(similarity, label)\n",
    "    loss = loss.to(device)\n",
    "    # print(f'Iteration {i}/{len(dataset)} | \\tLabel: {label} - Similarity: {similarity} - Loss: {loss}')\n",
    "    loss /= BATCH_SIZE\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def train_one_batch(model, training_set, optimizer, scheduler):\n",
    "    total_loss = 0\n",
    "    for i in range(len(training_set)):         \n",
    "        gop1, gop2, label = training_set[i]\n",
    "        loss = train_one_step(model, gop1, gop2, label)\n",
    "        total_loss += loss.item()\n",
    "        wb.log({\"instantaneous-loss\": loss.item()})\n",
    "        wb.log({\"total-loss\": total_loss})\n",
    "        wb.log({\"learning-rate\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "        if i % BATCH_SIZE == 0 or i == len(training_set) - 1:\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "    return total_loss\n",
    "\n",
    "def train_one_epoch(model, devices, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, training_set_devices in enumerate(devices):\n",
    "        print(f'Loading training set {i+1}/{len(devices)}')\n",
    "        training_set = GopPairDataset(vision_gop_dataset, N_GOPS_FROM_SAME_DEVICE, N_GOPS_FROM_DIFFERENT_DEVICE, consider_devices=training_set_devices, shuffle=True)\n",
    "        print(f'Training batch {i+1}/{len(devices)}')\n",
    "        train_one_batch(model, training_set, optimizer, scheduler)\n",
    "\n",
    "    print(f'Updating learning rate from {scheduler.get_last_lr()[0]}', end='')\n",
    "    scheduler.step()\n",
    "    print(f' to {scheduler.get_last_lr()[0]}')\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def validate_one_step(model, gop1, gop2, label):\n",
    "    gop1_features = model(gop1, debug=False, device=device)\n",
    "    gop2_features = model(gop2, debug=False, device=device)\n",
    "    gop1_features = gop1_features.to(device)\n",
    "    gop2_features = gop2_features.to(device)\n",
    "\n",
    "    similarity = compute_similarity(gop1_features, gop2_features).double()\n",
    "    similarity.to(device)\n",
    "    \n",
    "    label = torch.tensor(label, dtype=float, requires_grad=False, device=device)\n",
    "    label = label.double()\n",
    "    \n",
    "    loss = compute_loss(similarity, label)\n",
    "\n",
    "    return loss, label, similarity\n",
    "\n",
    "def validate_one_epoch(model, devices):\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    similarities = []\n",
    "    for i, testing_set_devices in enumerate(devices):\n",
    "        print(f'Loading validation set {i+1}/{len(devices)}')\n",
    "        testing_set = GopPairDataset(vision_gop_dataset, N_GOPS_FROM_SAME_DEVICE, N_GOPS_FROM_DIFFERENT_DEVICE, consider_devices=testing_set_devices, shuffle=True)\n",
    "        testing_set.pair_dataset = testing_set.pair_dataset[:floor(len(testing_set)*(1-TEST_PERCENTAGE/100))] # reduce size by removing TEST_PERCENTAGE % of the dataset\n",
    "        validation_set = copy.deepcopy(testing_set)\n",
    "        validation_set.pair_dataset = validation_set.pair_dataset[:floor(len(testing_set)*(1-VALIDATION_PERCENTAGE/100))] # VALIDATION_PERCENTAGE % of the training set is used for validation, works because the dataset is shuffled\n",
    "        testing_set.pair_dataset = testing_set.pair_dataset[floor(len(testing_set)*(1-VALIDATION_PERCENTAGE/100)):]\n",
    "        print(f'Validating batch {i+1}/{len(devices)}')\n",
    "        for j in range(len(validation_set)):\n",
    "            gop1, gop2, label = validation_set[j]\n",
    "            loss, label, similarity = validate_one_step(model, gop1, gop2, label)\n",
    "            wb.log({\"validation-loss\": loss.item()})\n",
    "            labels.append(label.item())\n",
    "            similarities.append(similarity.item())\n",
    "\n",
    "    return labels, similarities\n",
    "\n",
    "def compute_ROC(scores, labels, show: bool = True):\n",
    "    # compute ROC\n",
    "    fpr, tpr, thr = roc_curve(np.asarray(labels), np.asarray(scores), drop_intermediate=False)\n",
    "    # compute AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    if show is True:\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "\n",
    "        plt.plot(fpr, tpr, color='darkorange',\n",
    "                 lw=lw, label='AUC = %0.2f' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([-0.01, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    idx_tpr = np.where((fpr - TARGET_FPR) == min(i for i in (fpr - TARGET_FPR) if i > 0))\n",
    "    print('For a FPR approximately equals to %0.2f corresponds a TPR equal to %0.2f and a threshold equal to %0.4f with FPR equal to %0.2f' % (TARGET_FPR, tpr[idx_tpr[0][0]], thr[idx_tpr[0][0]], fpr[idx_tpr[0][0]]))\n",
    "\n",
    "    return thr[idx_tpr[0][0]], tpr[idx_tpr[0][0]], fpr[idx_tpr[0][0]], roc_auc # return thr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_GOPS_FROM_DIFFERENT_DEVICE = 3\n",
    "# N_GOPS_FROM_SAME_DEVICE = 3\n",
    "\n",
    "wb.init(project='h4vdm', config={\"learning-rate\": LEARNING_RATE,\n",
    "                                 \"learning-rate-decay-factor\": LEARNING_RATE_DECAY_FACTOR,\n",
    "                                 \"start-linear-learning-rate-coefficient\": START_LINEAR_LEARNING_RATE_COEFFICIENT,\n",
    "                                 \"end-linear-learning-rate-coefficient\": END_LINEAR_LEARNING_RATE_COEFFICIENT,\n",
    "                                 \"n-warmup-epochs\": WARM_UP_EPOCHS,\n",
    "                                 \"n_epochs\": n_datasets,\n",
    "                                 \"n-gops-from-same-device\": N_GOPS_FROM_SAME_DEVICE,\n",
    "                                 \"n-gops-from-different-device\": N_GOPS_FROM_DIFFERENT_DEVICE,\n",
    "                                 \"validation-percentage\": VALIDATION_PERCENTAGE,\n",
    "                                 \"test-percentage\": TEST_PERCENTAGE,\n",
    "                                 \"batch-size\": BATCH_SIZE,\n",
    "                                 \"target-fpr\": TARGET_FPR,\n",
    "                                 \"auc-increase-threshold\": AUC_INCREASE_THRESHOLD})\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, START_LINEAR_LEARNING_RATE_COEFFICIENT, END_LINEAR_LEARNING_RATE_COEFFICIENT, WARM_UP_EPOCHS)\n",
    "\n",
    "print('Starting warmup')\n",
    "for epoch in range(WARM_UP_EPOCHS - 1):\n",
    "    print(f'Warmup epoch {epoch+1}/{WARM_UP_EPOCHS}')\n",
    "    \n",
    "    epoch_loss = train_one_epoch(net, training_set_devices, optimizer, scheduler)\n",
    "    \n",
    "    labels, similarities = validate_one_epoch(net, testing_set_devices)\n",
    "\n",
    "    threshold, tpr, fpr, roc_auc = compute_ROC(similarities, labels)\n",
    "    f1 = f1_score(labels, (np.asarray(similarities) > threshold), pos_label=SAME_DEVICE_LABEL)\n",
    "    accuracy = (np.asarray(labels) == (np.asarray(similarities) > threshold)).mean()\n",
    "    wb.log({\"roc-threshold\": threshold})\n",
    "    wb.log({\"roc-tpr\": tpr})\n",
    "    wb.log({\"roc-fpr\": fpr})\n",
    "    wb.log({\"roc-auc\": roc_auc})\n",
    "    wb.log({\"f1-score\": f1})\n",
    "    wb.log({\"accuracy\": accuracy})\n",
    "\n",
    "    print(f'Warmup epoch {epoch+1}/{WARM_UP_EPOCHS} done')\n",
    "    print('')\n",
    "print('Warmup done\\n')\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, LEARNING_RATE_DECAY_FACTOR)\n",
    "\n",
    "old_roc_auc = 0\n",
    "epoch = 0\n",
    "auc_stable_counter = 0\n",
    "while True:\n",
    "    old_roc_auc = roc_auc\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    \n",
    "    epoch_loss = train_one_epoch(net, training_set_devices, optimizer, scheduler)\n",
    "    \n",
    "    labels, similarities = validate_one_epoch(net, testing_set_devices)\n",
    "\n",
    "    threshold, tpr, fpr, roc_auc = compute_ROC(similarities, labels)\n",
    "    f1 = f1_score(labels, (np.asarray(similarities) > threshold), pos_label=SAME_DEVICE_LABEL)\n",
    "    accuracy = (np.asarray(labels) == (np.asarray(similarities) > threshold)).mean()\n",
    "    wb.log({\"roc-threshold\": threshold})\n",
    "    wb.log({\"roc-tpr\": tpr})\n",
    "    wb.log({\"roc-fpr\": fpr})\n",
    "    wb.log({\"roc-auc\": roc_auc})\n",
    "    wb.log({\"f1-score\": f1})\n",
    "    wb.log({\"accuracy\": accuracy})\n",
    "\n",
    "    print(f'Epoch {epoch+1} done')\n",
    "    print('')\n",
    "\n",
    "    if abs(roc_auc - old_roc_auc) > AUC_INCREASE_THRESHOLD:\n",
    "        auc_stable_counter += 1\n",
    "    if auc_stable_counter > AUC_STABLE_COUNTER_THRESHOLD:\n",
    "        print(f'ROC AUC has not increased for {auc_stable_counter} epochs. Training terminated.')\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "print(f'Training terminated after {epoch} epochs')\n",
    "print(f'|New ROC AUC: {roc_auc} - Old ROC AUC: {old_roc_auc}| = |{roc_auc - old_roc_auc}| > {AUC_INCREASE_THRESHOLD}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)\n",
    "filename = os.path.join('models', datetime.today().strftime('%Y-%m-%d_%H:%M') + '_h4vdm.pth')\n",
    "print(f'Saving model to {filename}')\n",
    "torch.save(net, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('2024-02-22_19-38_h4vdm.pth')\n",
    "net.eval()\n",
    "\n",
    "gop1, gop2, label = pair_dataset[10]\n",
    "gop1_features = net(gop1, debug=False, device=device)\n",
    "gop2_features = net(gop2, debug=False, device=device)\n",
    "\n",
    "similarity = compute_similarity(gop1_features, gop2_features)\n",
    "\n",
    "print(f'Gop1: {gop1.video_name} Gop2: {gop2.video_name} Label: {label} Similarity: {similarity}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
