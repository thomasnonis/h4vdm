{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "import copy\n",
    "import torch\n",
    "import wandb as wb\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import auc, f1_score, roc_curve\n",
    "from packages.video_utils import H264Extractor, Video\n",
    "from packages.constants import GOP_SIZE, FRAME_HEIGHT, FRAME_WIDTH, DATASET_ROOT, N_GOPS_FROM_DIFFERENT_DEVICE, N_GOPS_FROM_SAME_DEVICE, SAME_DEVICE_LABEL\n",
    "from packages.dataset import VisionGOPDataset, GopPairDataset\n",
    "from packages.common import create_custom_logger\n",
    "from packages.network import H4vdmNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h4vdm.ipynb - INFO - Using device: cuda\n",
      "h4vdm.ipynb - INFO - Using device: cuda\n",
      "h4vdm.ipynb - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATASET_ROOT):\n",
    "    raise Exception(f'Dataset root does not exist: {DATASET_ROOT}')\n",
    "\n",
    "log = create_custom_logger('h4vdm.ipynb')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log.info(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GOP dataset\n",
    "\n",
    "Remember to delete dataset.json if you want to add new devices/videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h4vdm.ipynb - INFO - Dataset was loaded.\n",
      "h4vdm.ipynb - INFO - Dataset was loaded.\n",
      "h4vdm.ipynb - INFO - Dataset was loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1914\n"
     ]
    }
   ],
   "source": [
    "bin_path = os.path.abspath(os.path.join(os.getcwd(), 'h264-extractor', 'bin'))\n",
    "h264_ext_bin = os.path.join(bin_path, 'h264dec_ext_info')\n",
    "h264_extractor = H264Extractor(bin_filename=h264_ext_bin, cache_dir=DATASET_ROOT)\n",
    "Video.set_h264_extractor(h264_extractor)\n",
    "\n",
    "vision_gop_dataset = VisionGOPDataset(\n",
    "    root_path=DATASET_ROOT,\n",
    "    devices=[],\n",
    "    media_types = ['videos'],\n",
    "    properties=[],\n",
    "    extensions=['mp4', 'mov', '3gp'],\n",
    "    gop_size=GOP_SIZE,\n",
    "    frame_width=FRAME_WIDTH,\n",
    "    frame_height=FRAME_HEIGHT,\n",
    "    gops_per_video=4,\n",
    "    build_on_init=False,\n",
    "    force_rebuild=False,\n",
    "    download_on_init=False,\n",
    "    ignore_local_dataset=False,\n",
    "    shuffle=False)\n",
    "\n",
    "is_loaded = vision_gop_dataset.load()\n",
    "if not is_loaded:\n",
    "    log.info('Dataset was not loaded. Building...')\n",
    "else:\n",
    "    log.info('Dataset was loaded.')\n",
    "\n",
    "print(f'Dataset length: {len(vision_gop_dataset)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 devices: ['D01_Samsung_GalaxyS3Mini', 'D02_Apple_iPhone4s', 'D03_Huawei_P9', 'D04_LG_D290', 'D05_Apple_iPhone5c', 'D06_Apple_iPhone6', 'D07_Lenovo_P70A', 'D08_Samsung_GalaxyTab3', 'D09_Apple_iPhone4', 'D10_Apple_iPhone4s', 'D11_Samsung_GalaxyS3', 'D12_Sony_XperiaZ1Compact', 'D13_Apple_iPad2', 'D14_Apple_iPhone5c', 'D15_Apple_iPhone6', 'D16_Huawei_P9Lite', 'D17_Microsoft_Lumia640LTE', 'D18_Apple_iPhone5c', 'D19_Apple_iPhone6Plus', 'D20_Apple_iPadMini', 'D21_Wiko_Ridge4G', 'D22_Samsung_GalaxyTrendPlus', 'D23_Asus_Zenfone2Laser', 'D24_Xiaomi_RedmiNote3', 'D25_OnePlus_A3000', 'D26_Samsung_GalaxyS3Mini', 'D27_Samsung_GalaxyS5', 'D28_Huawei_P8', 'D29_Apple_iPhone5', 'D30_Huawei_Honor5c', 'D31_Samsung_GalaxyS4Mini', 'D32_OnePlus_A3003', 'D33_Huawei_Ascend', 'D34_Apple_iPhone5', 'D35_Samsung_GalaxyTabA']\n",
      "Training set 1 devices: ['D25_OnePlus_A3000', 'D15_Apple_iPhone6', 'D19_Apple_iPhone6Plus', 'D10_Apple_iPhone4s', 'D07_Lenovo_P70A', 'D35_Samsung_GalaxyTabA', 'D01_Samsung_GalaxyS3Mini', 'D21_Wiko_Ridge4G', 'D29_Apple_iPhone5', 'D30_Huawei_Honor5c', 'D04_LG_D290', 'D20_Apple_iPadMini', 'D34_Apple_iPhone5', 'D03_Huawei_P9', 'D16_Huawei_P9Lite', 'D23_Asus_Zenfone2Laser', 'D31_Samsung_GalaxyS4Mini', 'D14_Apple_iPhone5c']\n",
      "Testing set 1 devices: ['D18_Apple_iPhone5c', 'D13_Apple_iPad2', 'D28_Huawei_P8', 'D26_Samsung_GalaxyS3Mini', 'D02_Apple_iPhone4s', 'D32_OnePlus_A3003', 'D09_Apple_iPhone4', 'D24_Xiaomi_RedmiNote3', 'D22_Samsung_GalaxyTrendPlus', 'D05_Apple_iPhone5c', 'D27_Samsung_GalaxyS5', 'D06_Apple_iPhone6', 'D17_Microsoft_Lumia640LTE', 'D33_Huawei_Ascend', 'D11_Samsung_GalaxyS3', 'D08_Samsung_GalaxyTab3', 'D12_Sony_XperiaZ1Compact']\n",
      "Training set 2 devices: ['D14_Apple_iPhone5c', 'D11_Samsung_GalaxyS3', 'D29_Apple_iPhone5', 'D01_Samsung_GalaxyS3Mini', 'D31_Samsung_GalaxyS4Mini', 'D17_Microsoft_Lumia640LTE', 'D25_OnePlus_A3000', 'D08_Samsung_GalaxyTab3', 'D26_Samsung_GalaxyS3Mini', 'D02_Apple_iPhone4s', 'D19_Apple_iPhone6Plus', 'D10_Apple_iPhone4s', 'D32_OnePlus_A3003', 'D07_Lenovo_P70A', 'D05_Apple_iPhone5c', 'D04_LG_D290', 'D28_Huawei_P8', 'D23_Asus_Zenfone2Laser']\n",
      "Testing set 2 devices: ['D18_Apple_iPhone5c', 'D27_Samsung_GalaxyS5', 'D15_Apple_iPhone6', 'D12_Sony_XperiaZ1Compact', 'D30_Huawei_Honor5c', 'D33_Huawei_Ascend', 'D35_Samsung_GalaxyTabA', 'D34_Apple_iPhone5', 'D24_Xiaomi_RedmiNote3', 'D06_Apple_iPhone6', 'D22_Samsung_GalaxyTrendPlus', 'D20_Apple_iPadMini', 'D13_Apple_iPad2', 'D09_Apple_iPhone4', 'D03_Huawei_P9', 'D21_Wiko_Ridge4G', 'D16_Huawei_P9Lite']\n",
      "Training set 3 devices: ['D20_Apple_iPadMini', 'D24_Xiaomi_RedmiNote3', 'D01_Samsung_GalaxyS3Mini', 'D09_Apple_iPhone4', 'D21_Wiko_Ridge4G', 'D11_Samsung_GalaxyS3', 'D33_Huawei_Ascend', 'D03_Huawei_P9', 'D13_Apple_iPad2', 'D10_Apple_iPhone4s', 'D06_Apple_iPhone6', 'D31_Samsung_GalaxyS4Mini', 'D34_Apple_iPhone5', 'D08_Samsung_GalaxyTab3', 'D07_Lenovo_P70A', 'D28_Huawei_P8', 'D23_Asus_Zenfone2Laser', 'D15_Apple_iPhone6']\n",
      "Testing set 3 devices: ['D26_Samsung_GalaxyS3Mini', 'D32_OnePlus_A3003', 'D14_Apple_iPhone5c', 'D27_Samsung_GalaxyS5', 'D05_Apple_iPhone5c', 'D17_Microsoft_Lumia640LTE', 'D22_Samsung_GalaxyTrendPlus', 'D19_Apple_iPhone6Plus', 'D02_Apple_iPhone4s', 'D30_Huawei_Honor5c', 'D25_OnePlus_A3000', 'D12_Sony_XperiaZ1Compact', 'D04_LG_D290', 'D29_Apple_iPhone5', 'D18_Apple_iPhone5c', 'D35_Samsung_GalaxyTabA', 'D16_Huawei_P9Lite']\n",
      "Training set 4 devices: ['D10_Apple_iPhone4s', 'D01_Samsung_GalaxyS3Mini', 'D04_LG_D290', 'D07_Lenovo_P70A', 'D29_Apple_iPhone5', 'D30_Huawei_Honor5c', 'D28_Huawei_P8', 'D08_Samsung_GalaxyTab3', 'D05_Apple_iPhone5c', 'D09_Apple_iPhone4', 'D21_Wiko_Ridge4G', 'D35_Samsung_GalaxyTabA', 'D20_Apple_iPadMini', 'D17_Microsoft_Lumia640LTE', 'D23_Asus_Zenfone2Laser', 'D14_Apple_iPhone5c', 'D19_Apple_iPhone6Plus', 'D18_Apple_iPhone5c']\n",
      "Testing set 4 devices: ['D11_Samsung_GalaxyS3', 'D12_Sony_XperiaZ1Compact', 'D26_Samsung_GalaxyS3Mini', 'D15_Apple_iPhone6', 'D32_OnePlus_A3003', 'D03_Huawei_P9', 'D13_Apple_iPad2', 'D27_Samsung_GalaxyS5', 'D33_Huawei_Ascend', 'D22_Samsung_GalaxyTrendPlus', 'D34_Apple_iPhone5', 'D31_Samsung_GalaxyS4Mini', 'D25_OnePlus_A3000', 'D24_Xiaomi_RedmiNote3', 'D06_Apple_iPhone6', 'D16_Huawei_P9Lite', 'D02_Apple_iPhone4s']\n",
      "Training set 5 devices: ['D35_Samsung_GalaxyTabA', 'D11_Samsung_GalaxyS3', 'D07_Lenovo_P70A', 'D22_Samsung_GalaxyTrendPlus', 'D17_Microsoft_Lumia640LTE', 'D33_Huawei_Ascend', 'D18_Apple_iPhone5c', 'D13_Apple_iPad2', 'D25_OnePlus_A3000', 'D09_Apple_iPhone4', 'D34_Apple_iPhone5', 'D24_Xiaomi_RedmiNote3', 'D03_Huawei_P9', 'D23_Asus_Zenfone2Laser', 'D20_Apple_iPadMini', 'D12_Sony_XperiaZ1Compact', 'D02_Apple_iPhone4s', 'D04_LG_D290', 'D26_Samsung_GalaxyS3Mini', 'D01_Samsung_GalaxyS3Mini', 'D27_Samsung_GalaxyS5', 'D06_Apple_iPhone6', 'D05_Apple_iPhone5c', 'D30_Huawei_Honor5c']\n",
      "Testing set 5 devices: ['D14_Apple_iPhone5c', 'D15_Apple_iPhone6', 'D21_Wiko_Ridge4G', 'D19_Apple_iPhone6Plus', 'D16_Huawei_P9Lite', 'D08_Samsung_GalaxyTab3', 'D32_OnePlus_A3003', 'D29_Apple_iPhone5', 'D31_Samsung_GalaxyS4Mini', 'D28_Huawei_P8', 'D10_Apple_iPhone4s']\n",
      "Training set 6 devices: ['D14_Apple_iPhone5c', 'D15_Apple_iPhone6', 'D21_Wiko_Ridge4G', 'D19_Apple_iPhone6Plus', 'D16_Huawei_P9Lite', 'D08_Samsung_GalaxyTab3', 'D32_OnePlus_A3003', 'D29_Apple_iPhone5', 'D31_Samsung_GalaxyS4Mini', 'D28_Huawei_P8', 'D10_Apple_iPhone4s', 'D24_Xiaomi_RedmiNote3', 'D03_Huawei_P9', 'D23_Asus_Zenfone2Laser', 'D20_Apple_iPadMini', 'D12_Sony_XperiaZ1Compact', 'D02_Apple_iPhone4s', 'D04_LG_D290', 'D26_Samsung_GalaxyS3Mini', 'D01_Samsung_GalaxyS3Mini', 'D27_Samsung_GalaxyS5', 'D06_Apple_iPhone6', 'D05_Apple_iPhone5c', 'D30_Huawei_Honor5c']\n",
      "Testing set 6 devices: ['D35_Samsung_GalaxyTabA', 'D11_Samsung_GalaxyS3', 'D07_Lenovo_P70A', 'D22_Samsung_GalaxyTrendPlus', 'D17_Microsoft_Lumia640LTE', 'D33_Huawei_Ascend', 'D18_Apple_iPhone5c', 'D13_Apple_iPad2', 'D25_OnePlus_A3000', 'D09_Apple_iPhone4', 'D34_Apple_iPhone5']\n",
      "Training set 7 devices: ['D14_Apple_iPhone5c', 'D15_Apple_iPhone6', 'D21_Wiko_Ridge4G', 'D19_Apple_iPhone6Plus', 'D16_Huawei_P9Lite', 'D08_Samsung_GalaxyTab3', 'D32_OnePlus_A3003', 'D29_Apple_iPhone5', 'D31_Samsung_GalaxyS4Mini', 'D28_Huawei_P8', 'D10_Apple_iPhone4s', 'D35_Samsung_GalaxyTabA', 'D11_Samsung_GalaxyS3', 'D07_Lenovo_P70A', 'D22_Samsung_GalaxyTrendPlus', 'D17_Microsoft_Lumia640LTE', 'D33_Huawei_Ascend', 'D18_Apple_iPhone5c', 'D13_Apple_iPad2', 'D25_OnePlus_A3000', 'D09_Apple_iPhone4', 'D34_Apple_iPhone5']\n",
      "Testing set 7 devices: ['D24_Xiaomi_RedmiNote3', 'D03_Huawei_P9', 'D23_Asus_Zenfone2Laser', 'D20_Apple_iPadMini', 'D12_Sony_XperiaZ1Compact', 'D02_Apple_iPhone4s', 'D04_LG_D290', 'D26_Samsung_GalaxyS3Mini', 'D01_Samsung_GalaxyS3Mini', 'D27_Samsung_GalaxyS5', 'D06_Apple_iPhone6', 'D05_Apple_iPhone5c', 'D30_Huawei_Honor5c']\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "devices = list(vision_gop_dataset.get_devices())\n",
    "\n",
    "print(f'{len(devices)} devices: {devices}')\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_1_devices = devices[:len(devices)//2]\n",
    "training_set_1_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_2_devices = devices[:len(devices)//2]\n",
    "training_set_2_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_3_devices = devices[:len(devices)//2]\n",
    "training_set_3_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_4_devices = devices[:len(devices)//2]\n",
    "training_set_4_devices = devices[len(devices)//2:]\n",
    "\n",
    "shuffle(devices)\n",
    "testing_set_5_devices = devices[:len(devices)//3]\n",
    "training_set_5_devices = devices[len(devices)//3:]\n",
    "\n",
    "testing_set_6_devices = devices[len(devices)//3:2*(len(devices)//3)]\n",
    "training_set_6_devices = devices[:len(devices)//3] + devices[2*(len(devices)//3):]\n",
    "\n",
    "testing_set_7_devices = devices[2*(len(devices)//3):]\n",
    "training_set_7_devices = devices[:2*(len(devices)//3)]\n",
    "\n",
    "training_set_devices = [training_set_1_devices, training_set_2_devices, training_set_3_devices, training_set_4_devices, training_set_5_devices, training_set_6_devices, training_set_7_devices]\n",
    "testing_set_devices = [testing_set_1_devices, testing_set_2_devices, testing_set_3_devices, testing_set_4_devices, testing_set_5_devices, testing_set_6_devices, testing_set_7_devices]\n",
    "\n",
    "# training_set_devices = [training_set_1_devices]\n",
    "# testing_set_devices = [testing_set_1_devices]\n",
    "\n",
    "assert len(training_set_devices) == len(testing_set_devices), 'There must be the same number of training and testing sets'\n",
    "n_datasets = len(training_set_devices)\n",
    "\n",
    "for epoch in range(n_datasets):\n",
    "    print(f'Training set {epoch+1} devices: {training_set_devices[epoch]}')\n",
    "    print(f'Testing set {epoch+1} devices: {testing_set_devices[epoch]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build all GOPs so that cache can be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for device in vision_gop_dataset.get_devices():\n",
    "#     for video_metadata in vision_gop_dataset.dataset[device]:\n",
    "#         video = vision_gop_dataset._get_video_from_metadata(video_metadata)\n",
    "#         gops = video.get_gops()\n",
    "\n",
    "#         Video.h264_extractor.clean_cache()\n",
    "#         video = None\n",
    "#         gops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same / Same: 0.0\n",
      "Same / Different: 100.0\n",
      "Different / Different: 0.0\n",
      "Different / Same: 100.0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 72\n",
    "LEARNING_RATE = 8e-6\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.97\n",
    "VALIDATION_PERCENTAGE = 12.5 # 1/8\n",
    "TEST_PERCENTAGE = 40\n",
    "WARM_UP_EPOCHS = 5\n",
    "START_LINEAR_LEARNING_RATE_COEFFICIENT = 1e-3 # basically zero\n",
    "END_LINEAR_LEARNING_RATE_COEFFICIENT = 1\n",
    "TARGET_FPR = 0.01\n",
    "AUC_INCREASE_THRESHOLD = 0.01\n",
    "AUC_STABLE_COUNTER_THRESHOLD = 3\n",
    "\n",
    "# define loss function\n",
    "compute_loss = torch.nn.BCELoss()\n",
    "# compute_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Check if the loss function is working\n",
    "print(f'Same / Same: {compute_loss(torch.tensor([1.0]), torch.tensor([1.0]))}')\n",
    "print(f'Same / Different: {compute_loss(torch.tensor([1.0]), torch.tensor([0.0]))}')\n",
    "print(f'Different / Different: {compute_loss(torch.tensor([0.0]), torch.tensor([0.0]))}')\n",
    "print(f'Different / Same: {compute_loss(torch.tensor([0.0]), torch.tensor([1.0]))}')\n",
    "\n",
    "# instantiate the model\n",
    "net = H4vdmNet()\n",
    "# net = torch.load('models/2024-03-10_02:17_h4vdm.pth')\n",
    "net = net.to(device)\n",
    "  \n",
    "# instantiate the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def compute_similarity(gop1_features, gop2_features):\n",
    "    diff = torch.subtract(gop1_features, gop2_features)\n",
    "    norm = torch.norm(diff, 2)\n",
    "    tanh = torch.tanh(norm)\n",
    "    return (torch.ones(tanh.shape) - tanh)\n",
    "\n",
    "def train_one_step(model, gop1, gop2, label):\n",
    "    gop1_features = model(gop1, debug=False, device=device)\n",
    "    gop2_features = model(gop2, debug=False, device=device)\n",
    "    gop1_features = gop1_features.to(device)\n",
    "    gop2_features = gop2_features.to(device)\n",
    "\n",
    "    similarity = compute_similarity(gop1_features, gop2_features).double()\n",
    "    similarity.to(device)\n",
    "    \n",
    "    label = torch.tensor(label, dtype=float, requires_grad=False, device=device)\n",
    "    label = label.double()\n",
    "    \n",
    "    loss = compute_loss(similarity, label)\n",
    "    loss = loss.to(device)\n",
    "    # print(f'Iteration {i}/{len(dataset)} | \\tLabel: {label} - Similarity: {similarity} - Loss: {loss}')\n",
    "    loss /= BATCH_SIZE\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def train_one_batch(model, training_set, optimizer, scheduler):\n",
    "    total_loss = 0\n",
    "    for i in range(len(training_set)):         \n",
    "        gop1, gop2, label = training_set[i]\n",
    "        loss = train_one_step(model, gop1, gop2, label)\n",
    "        total_loss += loss.item()\n",
    "        wb.log({\"instantaneous-loss\": loss.item()})\n",
    "        wb.log({\"total-loss\": total_loss})\n",
    "        wb.log({\"learning-rate\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "        if i % BATCH_SIZE == 0 or i == len(training_set) - 1:\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "    return total_loss\n",
    "\n",
    "def train_one_epoch(model, devices, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, training_set_devices in enumerate(devices):\n",
    "        print(f'Loading training set {i+1}/{len(devices)}')\n",
    "        training_set = GopPairDataset(vision_gop_dataset, N_GOPS_FROM_SAME_DEVICE, N_GOPS_FROM_DIFFERENT_DEVICE, consider_devices=training_set_devices, shuffle=True)\n",
    "        print(f'Training batch {i+1}/{len(devices)}')\n",
    "        train_one_batch(model, training_set, optimizer, scheduler)\n",
    "\n",
    "    print(f'Updating learning rate from {scheduler.get_last_lr()[0]}', end='')\n",
    "    scheduler.step()\n",
    "    print(f' to {scheduler.get_last_lr()[0]}')\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def validate_one_step(model, gop1, gop2, label):\n",
    "    gop1_features = model(gop1, debug=False, device=device)\n",
    "    gop2_features = model(gop2, debug=False, device=device)\n",
    "    gop1_features = gop1_features.to(device)\n",
    "    gop2_features = gop2_features.to(device)\n",
    "\n",
    "    similarity = compute_similarity(gop1_features, gop2_features).double()\n",
    "    similarity.to(device)\n",
    "    \n",
    "    label = torch.tensor(label, dtype=float, requires_grad=False, device=device)\n",
    "    label = label.double()\n",
    "    \n",
    "    loss = compute_loss(similarity, label)\n",
    "\n",
    "    return loss, label, similarity\n",
    "\n",
    "def validate_one_epoch(model, devices):\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    similarities = []\n",
    "    for i, testing_set_devices in enumerate(devices):\n",
    "        print(f'Loading validation set {i+1}/{len(devices)}')\n",
    "        testing_set = GopPairDataset(vision_gop_dataset, N_GOPS_FROM_SAME_DEVICE, N_GOPS_FROM_DIFFERENT_DEVICE, consider_devices=testing_set_devices, shuffle=True)\n",
    "        testing_set.pair_dataset = testing_set.pair_dataset[:floor(len(testing_set)*(1-TEST_PERCENTAGE/100))] # reduce size by removing TEST_PERCENTAGE % of the dataset\n",
    "        validation_set = copy.deepcopy(testing_set)\n",
    "        validation_set.pair_dataset = validation_set.pair_dataset[:floor(len(testing_set)*(1-VALIDATION_PERCENTAGE/100))] # VALIDATION_PERCENTAGE % of the training set is used for validation, works because the dataset is shuffled\n",
    "        testing_set.pair_dataset = testing_set.pair_dataset[floor(len(testing_set)*(1-VALIDATION_PERCENTAGE/100)):]\n",
    "        \n",
    "        print(f'Validating batch {i+1}/{len(devices)}')\n",
    "        for j in range(len(testing_set)):\n",
    "            gop1, gop2, label = testing_set[j]\n",
    "            loss, label, similarity = validate_one_step(model, gop1, gop2, label)\n",
    "            wb.log({\"validation-loss\": loss.item()})\n",
    "            labels.append(label.item())\n",
    "            similarities.append(similarity.item())\n",
    "\n",
    "    return labels, similarities\n",
    "\n",
    "def compute_ROC(scores, labels, show: bool = True):\n",
    "    # compute ROC\n",
    "    fpr, tpr, thresholds = roc_curve(np.asarray(labels), np.asarray(scores), drop_intermediate=False)\n",
    "    # compute AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    tnr = 1 - fpr\n",
    "    max_index = np.argmax(tpr + tnr)\n",
    "\n",
    "    threshold = thresholds[max_index]\n",
    "    chosen_tpr = tpr[max_index]\n",
    "    chosen_fpr = fpr[max_index]\n",
    "\n",
    "\n",
    "    if show is True:\n",
    "        lw = 2\n",
    "        plt.figure()\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC (AUC = %0.2f)' % (roc_auc))\n",
    "        plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "        plt.plot(chosen_fpr, chosen_tpr, 'o', markersize=10, alpha=0.5, label=\"Threshold = %0.2f\" % threshold)\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    return threshold, chosen_tpr, chosen_fpr, roc_auc\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(scores, labels, threshold):\n",
    "    thresholded_scores = scores > threshold\n",
    "\n",
    "    precision = precision_score(labels, thresholded_scores, average='macro')\n",
    "    recall = recall_score(labels, thresholded_scores, average='macro')\n",
    "    f1 = f1_score(labels, thresholded_scores, average='macro')\n",
    "    accuracy = accuracy_score(labels, thresholded_scores)\n",
    "\n",
    "    return precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_GOPS_FROM_DIFFERENT_DEVICE = 3\n",
    "# N_GOPS_FROM_SAME_DEVICE = 3\n",
    "\n",
    "wb.init(project='h4vdm', config={\"learning-rate\": LEARNING_RATE,\n",
    "                                 \"learning-rate-decay-factor\": LEARNING_RATE_DECAY_FACTOR,\n",
    "                                 \"start-linear-learning-rate-coefficient\": START_LINEAR_LEARNING_RATE_COEFFICIENT,\n",
    "                                 \"end-linear-learning-rate-coefficient\": END_LINEAR_LEARNING_RATE_COEFFICIENT,\n",
    "                                 \"n-warmup-epochs\": WARM_UP_EPOCHS,\n",
    "                                 \"n_epochs\": n_datasets,\n",
    "                                 \"n-gops-from-same-device\": N_GOPS_FROM_SAME_DEVICE,\n",
    "                                 \"n-gops-from-different-device\": N_GOPS_FROM_DIFFERENT_DEVICE,\n",
    "                                 \"validation-percentage\": VALIDATION_PERCENTAGE,\n",
    "                                 \"test-percentage\": TEST_PERCENTAGE,\n",
    "                                 \"batch-size\": BATCH_SIZE,\n",
    "                                 \"target-fpr\": TARGET_FPR,\n",
    "                                 \"auc-increase-threshold\": AUC_INCREASE_THRESHOLD})\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, START_LINEAR_LEARNING_RATE_COEFFICIENT, END_LINEAR_LEARNING_RATE_COEFFICIENT, WARM_UP_EPOCHS)\n",
    "\n",
    "print('Starting warmup')\n",
    "for epoch in range(WARM_UP_EPOCHS - 1):\n",
    "    print(f'Warmup epoch {epoch+1}/{WARM_UP_EPOCHS}')\n",
    "    \n",
    "    epoch_loss = train_one_epoch(net, training_set_devices, optimizer, scheduler)\n",
    "    \n",
    "    labels, similarities = validate_one_epoch(net, testing_set_devices)\n",
    "\n",
    "    threshold, tpr, fpr, roc_auc = compute_ROC(similarities, labels)\n",
    "    precision, recall, f1, accuracy = compute_metrics(similarities, labels, threshold)\n",
    "\n",
    "    wb.log({\"roc-threshold\": threshold})\n",
    "    wb.log({\"roc-tpr\": tpr})\n",
    "    wb.log({\"roc-fpr\": fpr})\n",
    "    wb.log({\"roc-auc\": roc_auc})\n",
    "    wb.log({\"f1-score\": f1})\n",
    "    wb.log({\"accuracy\": accuracy})\n",
    "    wb.log({\"precision\": precision})\n",
    "    wb.log({\"recall\": recall})\n",
    "\n",
    "    print(f'Warmup epoch {epoch+1}/{WARM_UP_EPOCHS} done')\n",
    "    print('')\n",
    "print('Warmup done\\n')\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, LEARNING_RATE_DECAY_FACTOR)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, LEARNING_RATE_DECAY_FACTOR, last_epoch=19)\n",
    "\n",
    "old_roc_auc = 0\n",
    "auc_stable_counter = 0\n",
    "while True:\n",
    "    old_roc_auc = roc_auc\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    \n",
    "    epoch_loss = train_one_epoch(net, training_set_devices, optimizer, scheduler)\n",
    "    \n",
    "    labels, similarities = validate_one_epoch(net, testing_set_devices)\n",
    "\n",
    "    threshold, tpr, fpr, roc_auc = compute_ROC(similarities, labels)\n",
    "    precision, recall, f1, accuracy = compute_metrics(similarities, labels, threshold)\n",
    "\n",
    "    wb.log({\"roc-threshold\": threshold})\n",
    "    wb.log({\"roc-tpr\": tpr})\n",
    "    wb.log({\"roc-fpr\": fpr})\n",
    "    wb.log({\"roc-auc\": roc_auc})\n",
    "    wb.log({\"f1-score\": f1})\n",
    "    wb.log({\"accuracy\": accuracy})\n",
    "    wb.log({\"precision\": precision})\n",
    "    wb.log({\"recall\": recall})\n",
    "\n",
    "    print(f'Epoch {epoch+1} done')\n",
    "    print('')\n",
    "\n",
    "    if abs(roc_auc - old_roc_auc) > AUC_INCREASE_THRESHOLD:\n",
    "        auc_stable_counter += 1\n",
    "    if auc_stable_counter > AUC_STABLE_COUNTER_THRESHOLD:\n",
    "        print(f'ROC AUC has not increased for {auc_stable_counter} epochs. Training terminated.')\n",
    "        print(f'|New ROC AUC: {roc_auc} - Old ROC AUC: {old_roc_auc}| = |{roc_auc - old_roc_auc}| > {AUC_INCREASE_THRESHOLD}')\n",
    "        break\n",
    "\n",
    "    if not os.path.exists('config.txt'):\n",
    "        file = open(\"config.txt\", \"w\")\n",
    "        file.close()\n",
    "    with open('config.txt', 'r') as file:\n",
    "        first_line = file.readline()\n",
    "        if 'stop' in first_line:\n",
    "            print(\"Stopping...\")\n",
    "            break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "print(f'Training terminated after {epoch} epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('models', datetime.today().strftime('%Y-%m-%d_%H:%M') + '_h4vdm.pth')\n",
    "print(f'Saving model to {filename}')\n",
    "torch.save(net, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
