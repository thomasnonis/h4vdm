I_DONT_KNOW_WHAT_TO_DO_WITH_THIS = 0

# Paths
DATASET_ROOT = '/media/thomas/TN_SSD/h4vdm_datasets/VISION'

# Constants
MACROBLOCK_SIZE = 16

# Feature extraction
GOP_SIZE = 8 # L in paper

FRAME_HEIGHT = 224
FRAME_WIDTH = FRAME_HEIGHT

# Neural network hyperparameters
INTERMEDIATE_OUTPUTS_DIMENSION = 256 #D_0 in paper

VIT1_DEPTH = 8
VIT1_PROJECTION_DIMENSION = 256 #D_ViT1 in paper
VIT1_NUM_HEADS = 8
VIT1_MLP_DIMENSION = 4 * VIT1_PROJECTION_DIMENSION #as seen from other pytorch implementations, it is usually like this
VIT1_OUTPUT_DIMENSION = INTERMEDIATE_OUTPUTS_DIMENSION
VIT1_PATCH_SIZE = 16

VIT2_DEPTH = 4
VIT2_PROJECTION_DIMENSION = 64 #D_ViT2 in paper
VIT2_NUM_HEADS = 4
VIT2_MLP_DIMENSION = 4 * VIT2_PROJECTION_DIMENSION #as seen from other pytorch implementations, it is usually like this
VIT2_OUTPUT_DIMENSION = INTERMEDIATE_OUTPUTS_DIMENSION 
VIT2_PATCH_SIZE = 16

EMBEDDING_VOCABULARY_SIZE = GOP_SIZE
EMBEDDING_DIMENSION = INTERMEDIATE_OUTPUTS_DIMENSION

# jan_input_size = 4 * L + 5
JAN_INPUT_SIZE = 4 * GOP_SIZE + 5
JAN_N_HEADS = 8 # It is just the default value in PyTorch, not sure what to choose
JAN_N_LAYERS = 8

OUTPUT_DIMENSION = 1024 #D_r in paper